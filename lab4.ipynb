{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the tourism industry wants the ho chi minh city government to revive the nighttime economy with more amusement options and later closure of businesses', 'ngo thanh thuy of rex hotel in district 1 said at a meeting with city authorities thursday that le loi street one of the busiest tourism areas lacks nighttime amusement activities to engage foreign visitors', 'after the street is cleared of the metro construction site they should turn it into a street food hub with entertainment shows to help widen nighttime travel experiences he said', 'huynh phan phuong hoang deputy general director of tour operator vietravel too called on the city to open street food hubs and launch more nighttime entertainment to give foreigners more reason to extend their stay in vietnams premier metropolis', 'le quynh thu ceo of apex multimedia said the city has abundant resources to develop its nighttime economy and authorities should allow some businesses to stay open until 4 am', 'hcmc has two pedestrianonly plazas and a street that is offlimits to vehicles', 'the plazas are near the quang trung monument in district 10 and on the center strip on nguyen hue street', 'bui vien street is pedestrianonly from 7 pm during weekends', 'this year the city has received 21 million foreign visitors or 60 of its target of 35 million', 'in 2019 the year before covid began it had received over 86 million or nearly half the total foreign arrivals in vietnam']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import re\n",
    "\n",
    "pun = string.punctuation\n",
    "data = open('data/lab4.txt', encoding='utf8').read().splitlines()\n",
    "process_data = []\n",
    "for d in data:\n",
    "    d = d.lower()\n",
    "    d = re.sub(r'[^a-zA-Z0-9\\s]', '', d)\n",
    "    process_data.append(d)\n",
    "print(process_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot Vector (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "count = 0\n",
    "for doc in process_data:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = count\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(somestring):\n",
    "    onehot_encoded = []\n",
    "    for word in somestring.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if word in vocab:\n",
    "            temp[vocab[word]-1] = 1\n",
    "        onehot_encoded.append(temp)\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(get_onehot_vector('Ho Chi Minh City'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow representation for 'Ho Chi Minh City': [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "(1, 148)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# print(\"Our corpus: \", process_data)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(process_data)\n",
    "\n",
    "# print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"Ho Chi Minh City\"])\n",
    "print(\"Bow representation for 'Ho Chi Minh City':\", temp.toarray())\n",
    "print(temp.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary size:  577\n",
      "Bow representation for 'Ho Chi Minh City': [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 577)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(process_data)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "# print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "print(\"Our vocabulary size: \", len(count_vect.vocabulary_))\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"Ho Chi Minh City\"])\n",
    "\n",
    "print(\"Bow representation for 'Ho Chi Minh City':\", temp.toarray())\n",
    "temp.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 1.6061358  2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 1.6061358  2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.29928298 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.29928298 2.01160091 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.01160091 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 1.78845736 2.70474809 2.70474809\n",
      " 2.01160091 2.29928298 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 1.6061358  1.45198512 2.70474809 2.29928298\n",
      " 2.70474809 2.29928298 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.29928298 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.01160091 2.29928298 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 1.45198512 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 1.2006707  2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 1.45198512 2.70474809 2.70474809 2.70474809 2.29928298\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809\n",
      " 2.70474809 2.70474809 2.01160091 2.29928298]\n",
      "----------\n",
      "All words in the vocabulary ['10', '2019', '21', '35', '60', '86', 'abundant', 'activities', 'after', 'allow', 'am', 'amusement', 'and', 'apex', 'are', 'areas', 'arrivals', 'at', 'authorities', 'before', 'began', 'bui', 'busiest', 'businesses', 'called', 'center', 'ceo', 'chi', 'city', 'cleared', 'closure', 'construction', 'covid', 'deputy', 'develop', 'director', 'district', 'during', 'economy', 'engage', 'entertainment', 'experiences', 'extend', 'food', 'foreign', 'foreigners', 'from', 'general', 'give', 'government', 'had', 'half', 'has', 'hcmc', 'he', 'help', 'ho', 'hoang', 'hotel', 'hub', 'hubs', 'hue', 'huynh', 'in', 'industry', 'into', 'is', 'it', 'its', 'lacks', 'later', 'launch', 'le', 'loi', 'meeting', 'metro', 'metropolis', 'million', 'minh', 'monument', 'more', 'multimedia', 'near', 'nearly', 'ngo', 'nguyen', 'nighttime', 'of', 'offlimits', 'on', 'one', 'open', 'operator', 'options', 'or', 'over', 'pedestrianonly', 'phan', 'phuong', 'plazas', 'pm', 'premier', 'quang', 'quynh', 'reason', 'received', 'resources', 'revive', 'rex', 'said', 'should', 'shows', 'site', 'some', 'stay', 'street', 'strip', 'target', 'thanh', 'that', 'the', 'their', 'they', 'this', 'thu', 'thursday', 'thuy', 'to', 'too', 'total', 'tour', 'tourism', 'travel', 'trung', 'turn', 'two', 'until', 'vehicles', 'vien', 'vietnam', 'vietnams', 'vietravel', 'visitors', 'wants', 'weekends', 'widen', 'with', 'year']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.         0.         0.         ... 0.         0.17740979 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.14938477 0.        ]\n",
      " [0.         0.         0.         ... 0.21119437 0.15707148 0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.26432666 ... 0.         0.         0.22470181]\n",
      " [0.         0.23122678 0.         ... 0.         0.         0.19656389]]\n",
      "----------\n",
      "Tfidf representation for 'Ho Chi Minh City':\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.54614458 0.32431204 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.54614458 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.54614458 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(process_data)\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([\"Ho Chi Minh City\"])\n",
    "print(\"Tfidf representation for 'Ho Chi Minh City':\\n\", temp.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
